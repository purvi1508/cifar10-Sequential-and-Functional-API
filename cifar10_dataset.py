# -*- coding: utf-8 -*-
"""cifar10 dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tEPfd1i6c_S2ljCoJhDbsLsu75M-OV-K
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import cifar10

#50000training images
#10000test images
#32*32 pixels RGB
(x_train,y_train),(x_test,y_test)=cifar10.load_data()
x_train=x_train.astype("float32")/255.0
x_test=x_test.astype("float32")/255.0

model = tf.keras.models.Sequential([
  tf.keras.Input(shape=(32,32,3)),#this is the input shape of each image
  layers.Conv2D(32,3,padding='same', activation='relu'),
  layers.MaxPooling2D(pool_size=(2,2)),
  layers.Conv2D(64,3,padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(128,3,padding='same', activation='relu'),
  layers.Flatten(),
  layers.Dense(64,activation='relu'),
  layers.Dense(10)
])

model.compile(
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
optimizer=keras.optimizers.Adam(learning_rate=3e-4),
metrics=["accuracy"]
)

model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=2)
model.evaluate(x_train,y_train,batch_size=64,verbose=2)

#Functional API
def my_models():
  input=keras.Input(shape=(32,32,3))
  #covariant shift if we learn something from XY mapping if the distribution of X changes there might be the need to retrain the model
  x=layers.Conv2D(32,2)(input)
  x=layers.BatchNormalization()(x)
  x=keras.activations.relu(x)
  x=layers.MaxPooling2D()(x)
  x=layers.Conv2D(64,5,padding='same')(x)
  x=layers.BatchNormalization()(x)
  x=keras.activations.relu(x)
  x=layers.Conv2D(128,2)(x)
  x=layers.BatchNormalization()(x)
  x=keras.activations.relu(x)
  x=layers.Flatten()(x)
  x=layers.Dense(64,activation='relu')(x)
  output=layers.Dense(10)(x)
  model= keras.Model(inputs=input, outputs=output)
  return model

model= my_models()

model.compile(
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
optimizer=keras.optimizers.Adam(learning_rate=3e-4),
metrics=["accuracy"]
)

model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=2)
model.evaluate(x_train,y_train,batch_size=64,verbose=2)

#when the model is overfitting regulization is required  with L2 and Dropout
#Functional API
def my_models():
  input=keras.Input(shape=(32,32,3))
  #covariant shift if we learn something from XY mapping if the distribution of X changes there might be the need to retrain the model
  x=layers.Conv2D(32,3,padding='same',kernel_regularizer=regularizers.l2(0.01))(input)
  x=layers.BatchNormalization()(x)
  x=keras.activations.relu(x)
  x=layers.MaxPooling2D()(x)
  x=layers.Conv2D(64,3,padding='same',kernel_regularizer=regularizers.l2(0.01))(x)
  x=layers.BatchNormalization()(x)
  x=keras.activations.relu(x)
  x=layers.Conv2D(128,3,padding='same',kernel_regularizer=regularizers.l2(0.01))(x)
  x=layers.BatchNormalization()(x)
  x=keras.activations.relu(x)
  x=layers.Flatten()(x)
  x=layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)
  x=layers.Dropout(0.5)(x)
  output=layers.Dense(10)(x)
  model= keras.Model(inputs=input, outputs=output)
  return model
#Using 3 methods l2, batchnormalization and dropout

